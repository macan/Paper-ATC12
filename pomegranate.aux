\relax 
\citation{facebook-haystack}
\citation{taobao-tfs}
\citation{trf}
\citation{facebook-haystack}
\citation{lustre}
\citation{pvfs}
\citation{ceph}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{cffs}
\citation{reiserfs}
\citation{giga}
\citation{ipdps09}
\citation{europar08}
\citation{cassandra}
\citation{hbase}
\citation{ycsb}
\citation{cffs}
\citation{ceph}
\citation{reiserfs}
\citation{giga}
\citation{skyfs}
\citation{eh}
\@writefile{toc}{\contentsline {section}{\numberline {2}Motivation and Background}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Related Works}{2}}
\citation{hfs}
\citation{lustre}
\citation{pvfs}
\citation{europar08}
\citation{ipdps09}
\citation{hotcloud}
\citation{gfs}
\citation{cstore}
\citation{cassandra}
\citation{hbase}
\@writefile{toc}{\contentsline {section}{\numberline {3}Design Issues}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Directory Model for N-Form Problem. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Imagea-s and imagea-m are the small and medium forms of imagea. Plain model is either (a) metadata costing or (b) difficult to use. Column oriented tabular model resolves both these difficulties.}}}{3}}
\newlabel{directory_model}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tabular Directory Model}{3}}
\citation{lzo}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Sparse Table}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Advantages of CODIR}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Storage File Mappings. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The first three columns are grouped and aggregated as one storage file, the next two columns are aggregated as independent storage files, and the Tag column is saved in a sorted file. Boost File accelerates metadata access. Indirect file contains indirect columns' metadata.}}}{4}}
\newlabel{storage_engine_model}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Structured Storage}{4}}
\citation{lfs}
\citation{giga}
\citation{skyfs}
\citation{spyglass}
\citation{smartstore}
\citation{spyglass}
\citation{smartstore}
\citation{berkeleydb}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Aggregated Storage Files}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Structured Storage Files}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sorted File Format. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Columns of directory X and Y are directed to Sorted File $\alpha $. Each database contains one base file and several secondary indexes.}}}{5}}
\newlabel{index_mapping}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Tabular Query Interface}{5}}
\newlabel{interface}{{3.3}{5}}
\citation{fuse}
\citation{gfs}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces User Level Interface for File and Directory}}{6}}
\newlabel{tabular_interface_table}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Architecture of PFS}}{6}}
\newlabel{hvfs_architecture}{{4}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation Details}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Distinguish Small Files}{6}}
\citation{lustre}
\citation{gfs}
\citation{ceph}
\citation{giga}
\citation{pvfs}
\citation{giga}
\citation{ch}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Elastic Metadata Service}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Parallel Small File I/O}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces I/O Write Flow: Scatter and Aggregation}}{7}}
\newlabel{io_write_flow}{{5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Evaluation}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experimental Setup}{7}}
\citation{cluster_postmark}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Space consumption of saving 1.5 million data contents in PFS, Ext3, OrangeFS. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip By adopting CODIR, the metadata space consumption is only 1/20 comparing with PFS without CODIR. Meanwhile, CODIR uses less data and metadata space comparing with Ext3 and OrangeFS, and has the lowest latency for this test.}}}{8}}
\newlabel{spacesavingx}{{6}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}NForm Promblem}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces HTTP performance of a GIS Tile Server based on OrangeFS and PFS. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The served HTTP request rate of PFS with CODIR is twice of OrangeFS. With less metadata, file read is more faster.}}}{8}}
\newlabel{gis}{{7}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Structured Storage and Interface}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}File Aggregation}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Small File I/O Scalability in Range [1,1K]. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Small file write rates increase linearly with more nodes until the servers are saturated. Double the number of servers almost doubles the aggregated file write bandwidth. However, write bandwidth doesn't increase with more servers because of the limited network message rate.}}}{9}}
\newlabel{ioscale}{{8}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Small File Create Rates and Write Bandwidth. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip For OrangeFS, clients totally creates 4,700,000 files; while for PFS, clients totally creates 47,000,000 files. With increasing the file size, file create rate decreases. The aggregated write bandwidth of PFS with 512KB file is more than 1GB per second.}}}{9}}
\newlabel{smallcreates}{{9}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Small File Read/Append Bandwidth. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip File read and append bandwidth of PFS is larger than that of OrangeFS. The random read bandwidth of PFS drops at 8KB because the network MTU is only 5100B. Both read and append bandwidth drops at 16KB because PFS client uses 16KB buffer page. More pages means longer latency to read and write. The RMW issue in file append has little impaction on I/O bandwidth.}}}{9}}
\newlabel{smalliora}{{10}{9}}
\citation{gfs}
\citation{giga}
\citation{ceph}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Runtime and Space Consumption of Untar 1 million Gene Sequences. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Untar short gene sequences in PFS is fastest and most space efficient with respect to OrangeFS and Ceph. This test runs on one machine.}}}{10}}
\newlabel{spacesavingy}{{11}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Tag Set and Search Latency. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Using sorted file $+$ tabular query interface, tag set and search are faster than external database(BDB) and brute-force scanning search. Especially, external database approach is slow at importing tags to database. And for queries of lower selectivity, sorted file $+$ tabular query performs much more better.}}}{10}}
\newlabel{xattr}{{12}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Sorted File and Tabular Query}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Metadata Scalability of PFS. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip PFS delivers a peak throughput of roughly 180,000 file creates per second. The network message rate limits PFS's ability to match the ideal linear scalability. It is fast than GIGA+ because of Two-Stage split strategy and none out-of-core indexing.}}}{10}}
\newlabel{mdscale}{{13}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Elastic Metadata Service}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Local Only, Random, and Two-Stage Splits. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Local only splits couldn't exploit more nodes to improve service rate; Random splits introduces too many remote splits and network latency. Both are not efficient slice split strategy for large scale metadata service. Two-Stage splits performs much more better than them.}}}{11}}
\newlabel{splits}{{14}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Slice Split}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Elastic Speedup}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Cumulative Distribution Function of three Runs. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Online a server shifts the curve to lower latency(Conf A to B). Offline a server shifts the curve to higher latency(Conf B to C). It means that PFS scales up and down elastically.}}}{11}}
\newlabel{onlineoffline}{{15}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Relayed Large File I/O}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion and Future Work}{11}}
\bibcite{facebook-haystack}{1}
\bibcite{taobao-tfs}{2}
\bibcite{trf}{3}
\bibcite{bigtable}{4}
\bibcite{cstore}{5}
\bibcite{cassandra}{6}
\bibcite{hbase}{7}
\bibcite{ycsb}{8}
\bibcite{lfs}{9}
\bibcite{zfs}{10}
\bibcite{wafl}{11}
\bibcite{gossip}{12}
\bibcite{ch}{13}
\bibcite{reiserfs}{14}
\bibcite{hotcloud}{15}
\bibcite{eh}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Large File I/O Bandwidth of PFS. \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip With respect to Ext3, PFS FUSE performs bad on both sequential read and write, and introduce 33\% and 15\% overhead. However, comparing with SkyFS in large cluster, PFS performs better.}}}{12}}
\newlabel{largeio}{{16}{12}}
\bibcite{cffs}{17}
\bibcite{ceph}{18}
\bibcite{hfs}{19}
\bibcite{giga}{20}
\bibcite{skyfs}{21}
\bibcite{nisar}{22}
\bibcite{fuse}{23}
\bibcite{cluster_postmark}{24}
\bibcite{gfs}{25}
\bibcite{chord}{26}
\bibcite{spyglass}{27}
\bibcite{smartstore}{28}
\bibcite{lustre}{29}
\bibcite{pvfs}{30}
\bibcite{europar08}{31}
\bibcite{ipdps09}{32}
\bibcite{berkeleydb}{33}
\bibcite{lzo}{34}
\bibcite{dbfs}{35}
